---
date: 2024-10-06 19:53
tags:
  - "#inprogess"
source: https://chatgpt.com/share/6703155e-5924-8000-b41c-77c14fd7b255
---


# What is BERT?

- Stands for Bidirectional Encoder Representation from Transformers
- Create to enable models to understand the context of a word in a sentence more effectively
- Based on the transformed architecture (what's the transformer architecture?)
- The bidirectionality from BERT comes from the fact that it considers both left and right context of each word at the same time.
